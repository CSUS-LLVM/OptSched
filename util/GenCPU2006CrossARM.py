#!/usr/bin/env python3
"""
Process benchmark results generated by the running the CPU2006 benchmarks with
the RunCPU2006CrossARM script.
Example: ./RunCPU2006CrossARM 2>&1 | ./GenCPU2006CrossARM > result
TODO: Error checking on number of iterations. Make this value automatically discoverable.
"""
from __future__ import print_function
from optparse import OptionParser
from statistics import median, stdev
import sys
import re
from functools import reduce
from collections import OrderedDict
from abc import ABCMeta, abstractmethod

BENCH_START_REGEX = re.compile("Running (\S+)")
BENCH_USER_TIME_REGEX = re.compile("user\s+(\S+)")

# Statistics about the running time of a single benchmark
class BenchData:
    def __init__(self, name):
        self.name = name
        self.userTimes = []
        self.userTimeMedian = None
        self.userTimeStdev = None

    # Find the median runtime
    def findUserMedianTime(self):
        self.userTimeMedian = median(self.userTimes)

    # Find the standard deviation of the user times
    def findUserStdev(self):
        self.userTimeStdev = stdev(self.userTimes)

    # Find all statistics for user time
    def findAllUserTimeStats(self):
        self.findUserMedianTime()
        self.findUserStdev()

# Abstract class for printing benchmark stats.
class BenchStatsPrinter(metaclass=ABCMeta):
    @abstractmethod
    def printBenchStatsCSV(self):
        pass

# Print benchmark stats in csv format without comparing to a base set of results.
class BasicBenchStatsPrinter(BenchStatsPrinter):
    def __init__(self, iterations, peakBenchesData):
        self.iterations = iterations
        self.peakBenchesData = peakBenchesData

    # Print benchstats in csv format
    def printBenchStatsCSV(self):
        # Median benchmark runtimes
        medians = []

        print ("Benchmarks:,", end='')
        # Print benchmark names
        for entry in self.peakBenchesData:
            print (',' + self.peakBenchesData[entry].name, end='')

        # Print benchmark runtimes
        for i in range(self.iterations):
            print("\n,", end='')
            for entry in self.peakBenchesData:
                print(',' + str(round(self.peakBenchesData[entry].userTimes[i], 2)), end='')

        # Print median runtime
        print("\nMedian:,", end='')
        for entry in self.peakBenchesData:
            medians.append(self.peakBenchesData[entry].userTimeMedian)
            print(',' + str(round(self.peakBenchesData[entry].userTimeMedian, 2)), end='')

        # Print stdev
        print ("\nStandard Deviation:,", end='')
        for entry in self.peakBenchesData:
            print(',' + str(round(self.peakBenchesData[entry].userTimeStdev, 2)), end='')

        # Print %stdev
        print ("\n% Stdev (RSD):,", end='')
        for entry in self.peakBenchesData:
            print(',' + str(round(self.peakBenchesData[entry].userTimeStdev / self.peakBenchesData[entry].userTimeMedian * 100, 2)) + '%', end='')

        # Print geomean of medians
        print ("\nGeomean:,,", end='')
        print (round(reduce(lambda x, y: x*y, medians)**(1.0/len(medians)), 2), end='')

        # End
        print("\n", end='')

# Print benchmark stats in csv format, compare peak resutls with a baseline set
# of reference results.
class BCBenchStatsPrinter(BenchStatsPrinter):
    def __init__(self, iterations, peakBenchesData, baseBenchesData):
        self.iterations = iterations
        self.peakBenchesData = peakBenchesData
        self.baseBenchesData = baseBenchesData

    # Print benchstats in csv format
    def printBenchStatsCSV(self):
        # Median benchmark runtimes
        peakMedians = []
        baseMedians = []

        print ("Benchmarks:,", end='')
        # Print benchmark names
        for entry in self.peakBenchesData:
            print (',' + self.peakBenchesData[entry].name, end='')

        # Print benchmark runtimes
        for i in range(self.iterations):
            print("\n,", end='')
            for entry in self.peakBenchesData:
                print(',' + str(round(self.peakBenchesData[entry].userTimes[i], 2)), end='')

        # Print median runtime
        print("\nMedian:,", end='')
        for entry in self.peakBenchesData:
            peakMedians.append(self.peakBenchesData[entry].userTimeMedian)
            print(',' + str(round(self.peakBenchesData[entry].userTimeMedian, 2)), end='')

        # Print % improvement to base runtime
        print("\n% imp to base:,", end='')
        for entry in self.baseBenchesData:
            baseMedians.append(self.baseBenchesData[entry].userTimeMedian)
            improvement = (self.baseBenchesData[entry].userTimeMedian - self.peakBenchesData[entry].userTimeMedian) / self.baseBenchesData[entry].userTimeMedian
            print(',' + str(round(improvement * 100, 2)) + "%", end='')

        # Print stdev
        print ("\nStandard Deviation:,", end='')
        for entry in self.peakBenchesData:
            print(',' + str(round(self.peakBenchesData[entry].userTimeStdev, 2)), end='')

        # Print %stdev
        print ("\n% Stdev (RSD):,", end='')
        for entry in self.peakBenchesData:
            print(',' + str(round(self.peakBenchesData[entry].userTimeStdev / self.peakBenchesData[entry].userTimeMedian * 100, 2)) + '%', end='')

        peakGeomean = reduce(lambda x, y: x*y, peakMedians)**(1.0/len(peakMedians))
        baseGeomean = reduce(lambda x, y: x*y, baseMedians)**(1.0/len(baseMedians))

        # Print geomean of medians
        print ("\nGeomean:,,", end='')
        print (str(round(peakGeomean, 2)), end='')

        # Print % improvement of geomean compared to base
        print ("\nGeomean % imp:,,", end='')
        print (str(round((baseGeomean - peakGeomean) / baseGeomean * 100, 2)) + "%", end='')

        # End
        print("\n", end='')

# Find time in seconds from linux time utility default format (xxmx.xxs)
def parseTime(time):
    splitM = time.split('m')
    minutes = float(splitM[0])
    seconds = float(splitM[1][:-1])
    return 60.0 * minutes + seconds

# Find time in seconds from linux time utility default format (xxmx.xxs)
def parseTime(time):
    splitM = time.split('m')
    minutes = float(splitM[0])
    seconds = float(splitM[1][:-1])
    return 60.0 * minutes + seconds

# Find "BenchData" for a single benchmark
def parseBench(linesIter, benchData):
    for line in linesIter:
        # Look for the user runtime for the benchmark
        findBenchUserTime = BENCH_USER_TIME_REGEX.findall(line)
        if (findBenchUserTime != []):
            userTime = findBenchUserTime[0]
            benchData.userTimes.append(parseTime(userTime))
            return

    raise Exception("Could not find runtime for benchmark \"" + benchData.name + "\"")

# Find "BenchData" for all benchmarks within "inData"
def parseData(inData):
    benchesData = OrderedDict()
    linesIter = iter(inData)
    for line in linesIter:
        # Look for the start of a new benchmark
        findBenchDataStart = BENCH_START_REGEX.findall(line)
        if (findBenchDataStart != []):
            benchName = findBenchDataStart[0]
            if benchName not in benchesData:
                benchData = BenchData(benchName)
                benchesData[benchName] = benchData
            benchData = parseBench(linesIter, benchesData[benchName])

    return benchesData

def main(args):
    # Required test data to be proccessed. Supplied from stdin or a file via the -i flag.
    peakBenchesData = None
    # An optional reference test run file for comparison with purposes.
    baseBenchesData = None

    # Read input data from file or stdin
    try:
        # If argument to cl opt -i is '-' read from stdin
        if (args.infile == '-'):
            inData = sys.stdin
        # Otherwise Read data from file
        if (args.infile):
            inData = open(args.infile)
        # If there is no cl opt -i, default to reading from stdin
        elif (sys.stdin):
            inData = sys.stdin

        # Extract benchmark data
        peakBenchesData = parseData(inData)

        inData.close
    except IOError:
        print("Unable to parse input data.")
        raise

    # Check if there is a base results file for comparison
    if (args.basefile is not None):
        try:
            baseData = open(args.basefile)

            # Extract benchmark data
            baseBenchesData = parseData(baseData)
            baseData.close()
        except IOError:
            print ("Unable to parse basefile input data.")
            raise

    # Find usertime stats for peak bench data
    for entry in peakBenchesData:
        peakBenchesData[entry].findAllUserTimeStats()

    # Find usertime stats for base bench data
    if (baseBenchesData is not None):
        for entry in baseBenchesData:
            baseBenchesData[entry].findAllUserTimeStats()

    # Print stats in csv format
    csvPrinter = None
    if (baseBenchesData is not None):
        csvPrinter = BCBenchStatsPrinter(3, peakBenchesData, baseBenchesData)
    else:
        csvPrinter = BasicBenchStatsPrinter(3, peakBenchesData)

    assert isinstance(csvPrinter, BenchStatsPrinter)
    csvPrinter.printBenchStatsCSV()

if __name__ == '__main__':
    parser = OptionParser(
        description='Generate stats from running the CPU2006 benchmarks with RunCPU2006CrossARM')
    parser.add_option('-i', '--infile',
                      metavar='filepath',
                      help='Where to find the test run direcotires.')
    parser.add_option('-b', '--basefile',
                      metavar='filepath',
                      help='Where to find base test results for comparison')

    main(parser.parse_args()[0])
