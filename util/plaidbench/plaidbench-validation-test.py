# /usr/bin/python3
'''
**********************************************************************************
Description:    Validation script for OptSched with the plaidbench benchmarking
                suite. This script is meant to be used together with the
                run-plaidbench.sh script.
Modified by:    Vang Thao
Last Update:    December 17, 2019
**********************************************************************************

INPUT: Two directories generated by run-plaidbench.sh preferably  with different
    settings used.
    1.) Directory generated by run-plaidbench.sh
    2.) Directory generated by run-plaidbench.sh

OUTPUT: (To terminal)
    1.) Number of blocks in both file
    2.) Blocks that are optimal in both logs
    3.) Blocks that are optimal that are not optimal in the other log
    4.) Mismatches

HOW TO USE:
    1.) Run two plaidbench benchmarks with run-plaidbench.sh to generate
        two directories containing the results for each run.
    2.) Enter in the path to those directories as arguments to this script
'''

import argparse
import logging
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from readlogs import *

# Store DAGs stats for each benchmark and passes
dags = []
# Data structure
# Run number
# Benchmark Name
# Passthrough Number
# DAG name
# DAG stats
# dags[Run number][Benchmark name][Passthrough number][Dag name][DAG stats]

# Store number of DAGs
numDags = []

# Comparison function, takes in a pass number
# Valid arguments: 'first' or 'second'


def parseStats(filePaths):
    # Get logger
    logger = logging.getLogger('parseStats')

    # Temp variable to hold the benchmark's stats
    stats = {}

    # Collect DAG info from the two run results
    for i in range(0, 2):
        stats[i] = {}
        for bench in filePaths[i]:
            logger.debug('Verifying file {} exists'.format(
                filePaths[i][bench]))
            if os.path.exists(filePaths[i][bench]):
                logger.debug(
                    'File found! Processing file {}'.format(filePaths[i][bench]))
                # Open log file
                with open(filePaths[i][bench]) as file:
                    # Declare containers for this benchmark
                    curStats = {}
                    # Use a list instead of a dict to store results for each
                    # scheduling region to account for duplicate kernels. The
                    # order of the scheduling regions should not change between
                    # runs.
                    curStats['first'] = []
                    curStats['second'] = []
                    # Read and split scheduling regions
                    log = file.read()
                    blocks = split_blocks(log)
                    for block in blocks:
                        events = keep_only_first_event(parse_events(block))
                        # Get pass num
                        passNum = 'first'
                        if 'PassFinished' in events.keys():
                            passNumInt = events['PassFinished']['num']
                            if passNumInt == 1:
                                passNum = 'first'
                            elif passNumInt == 2:
                                passNum = 'second'

                        # Get DAG stats
                        dagName = events['BestResult']['name']
                        dagCost = events['BestResult']['cost'] + \
                            events['CostLowerBound']['cost']
                        dagLength = events['BestResult']['length']
                        dagIsOptimal = events['BestResult']['optimal']
                        dagInstrCount = events['ProcessDag']['num_instructions']

                        if events['BestResult']['cost'] < 0:
                            raise AssertionError("Cost is negative in the following block:\n" + block)

                        # Add this DAG's stats to temp stats container
                        curStats[passNum].append([
                            dagName,
                            dagCost,
                            dagLength,
                            dagIsOptimal,
                            dagInstrCount
                        ])

                    stats[i][bench] = curStats
            else:
                logger.warning(
                    'Cannot find log file for benchmark {}.'.format(bench))

    # printStats(stats)
    return stats


def printStats(stats):
    for index in stats:
        print('Index: {}'.format(index))
        for bench in stats[index]:
            print('  Bench: {}'.format(bench))
            for passNum in stats[index][bench]:
                print('    {} pass'.format(passNum.capitalize()))
                for region in stats[index][bench][passNum]:
                    print('      {}: dag cost: {}, length: {}, optimal: {}'.
                          format(region[0], region[1], region[2], region[3]))


def compareDags(displayMismatches, displayNumLargest, displayNumSmallest, stats, passNum):
    # Get logger
    logger = logging.getLogger('compareDags')

    totalDagCount1 = 0
    totalDagCount2 = 0

    # Compare the number of scheduling regions in each test. We require the
    # number of scheduling regions to be the same.
    for bench in stats[0]:
        totalDagCount1 += len(stats[0][bench][passNum])

    for bench in stats[1]:
        totalDagCount2 += len(stats[1][bench][passNum])

    if totalDagCount1 != totalDagCount2:
        logger.critical('Different number of dags in each log file.')
        sys.exit()

    # The number of blocks that are optimal in both logs.
    optimalInBoth = 0
    # The number of blocks that are only optimal in log 1.
    optimalLog1 = 0
    # The number of blocks that are only optimal in log 2.
    optimalLog2 = 0
    # Mismatches where blocks are optimal in both logs but have different costs.
    misNonEqual = 0
    # Mismatches where block is optimal in log 1 but it has a higher cost than the non-optimal block in log 2.
    misBlk1Opt = 0
    # Mismatches where block is optimal in log 2 but it has a higher cost than the non-optimal block in log 1.
    misBlk2Opt = 0
    # The quantity of blocks with the largest mismatches to print.
    numLarMisPrt = displayNumLargest
    # The quantity of mismatched blocks with the shortest length to print.
    numSmlBlkPrt = displayNumSmallest
    # Dictionary with the sizes of the mismatches for each mismatched block and the size of the block.
    mismatches = {}

    for bench in stats[0]:
        for i in range(len(stats[0][bench][passNum])):
            dag1 = stats[0][bench][passNum][i]
            dag2 = stats[1][bench][passNum][i]

            dagName1 = dag1[0]
            dagName2 = dag2[0]
            dagCost1 = dag1[1]
            dagCost2 = dag2[1]
            dagLength1 = dag1[2]
            dagLength2 = dag2[2]
            dagIsOptimal1 = dag1[3]
            dagIsOptimal2 = dag2[3]
            dagInstrCount1 = dag1[4]
            dagInstrCount2 = dag2[4]

            if dagName1 != dagName2 or dagInstrCount1 != dagInstrCount2:
                logger.critical(
                    'Processing {} in file 1 but could not find a match at the same location in file 2.'.format(dagName1))
                print('Expected {} with instr {} but found {} with instr {} instead'.format(
                    dagName1, dagInstrCount1, dagName2, dagInstrCount2))
                sys.exit()

            if dagIsOptimal1 and dagIsOptimal2:
                optimalInBoth += 1
                if dagCost1 != dagCost2:
                    # There was a mismatch where blocks are optimal in both logs but have different costs
                    misNonEqual += 1
                    mismatches[dagName1] = {}
                    mismatches[dagName1]['length'] = dagLength1
                    mismatches[dagName1]['misSize'] = abs(
                        dagCost1 - dagCost2)
                    # print('Mismatch for dag ' + dagName + ' (Both optimal with non-equal cost)')

            elif dagIsOptimal1:
                optimalLog1 += 1
                if dagCost1 > dagCost2:
                    # There was a mismatch where block is optimal in log 1 but it has a higher cost than the non-optimal block in log 2
                    misBlk1Opt += 1
                    mismatches[dagName1] = {}
                    mismatches[dagName1]['length'] = dagLength1
                    mismatches[dagName1]['misSize'] = dagCost1 - \
                        dagCost2
                    # print('Mismatch for dag ' + dagName + ' (Only optimal in log 1 but has higher cost than the non-optimal block in log 2)')

            elif dagIsOptimal2:
                optimalLog2 += 1
                if dagCost2 > dagCost1:
                    # There was a mismatch where block is optimal in log 2 but it has a higher cost than the non-optimal block in log 1
                    misBlk2Opt += 1
                    mismatches[dagName1] = {}
                    mismatches[dagName1]['length'] = dagLength1
                    mismatches[dagName1]['misSize'] = dagCost2 - \
                        dagCost1
                    # print('Mismatch for dag ' + dagName + ' (Only optimal in log 2 but has higher cost than the non-optimal block in log 1)')

    print('Optimal Block Stats for {} pass'.format(passNum))
    print('-----------------------------------------------------------')
    print('Blocks in log file 1: {}'.format(totalDagCount1))
    print('Blocks in log file 2: {}'.format(totalDagCount2))
    print('Blocks that are optimal in both files: ' + str(optimalInBoth))
    print('Blocks that are optimal in log 1 but not in log 2: ' + str(optimalLog1))
    print('Blocks that are optimal in log 2 but not in log 1: ' + str(optimalLog2))
    print('----------------------------------------------------------\n')

    print('Mismatch stats')
    print('-----------------------------------------------------------')
    print('Mismatches where blocks are optimal in both logs but have different costs: ' + str(misNonEqual))
    print('Mismatches where the block is optimal in log 1 but it has a higher cost than the non-optimal block in log 2: ' + str(misBlk1Opt))
    print('Mismatches where the block is optimal in log 2 but it has a higher cost than the non-optimal block in log 1: ' + str(misBlk2Opt))
    print('Total mismatches: ' + str(misNonEqual + misBlk1Opt + misBlk2Opt))
    print('-----------------------------------------------------------\n')

    if displayMismatches:
        if numLarMisPrt != 0:
            print('The ' + str(numLarMisPrt) +
                  ' mismatched blocks with the largest difference in cost')
            print('-----------------------------------------------------------')
            sortedMaxMis = sorted(mismatches.items(), key=lambda i: (
                mismatches[i[0]]['misSize'], i[0]), reverse=True)
            i = 1
            for block in sortedMaxMis[:numLarMisPrt]:
                print(str(i) + ':')
                print('Block Name: ' + block[0] + '\nLength: ' + str(
                    block[1]['length']) + '\nDifference in cost: ' + str(block[1]['misSize']))
                i += 1
            print('-----------------------------------------------------------\n')

        if numSmlBlkPrt != 0:
            print('The smallest ' + str(numSmlBlkPrt) + ' mismatched blocks')
            print('-----------------------------------------------------------')
            sortedMisSize = sorted(mismatches.items(), key=lambda i: (
                mismatches[i[0]]['length'], i[0]))
            i = 1
            for block in sortedMisSize[:numSmlBlkPrt]:
                print(str(i) + ':')
                print('Block Name: ' + block[0] + '\nLength: ' + str(
                    block[1]['length']) + '\nDifference in cost: ' + str(block[1]['misSize']))
                i += 1
            print('-----------------------------------------------------------')


def main(args):
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)

    filePaths1 = get_bench_log_paths(args.directory1, args.benchmark)
    filePaths2 = get_bench_log_paths(args.directory2, args.benchmark)

    filePaths = [filePaths1, filePaths2]

    stats = parseStats(filePaths)

    # Compare DAGs from first passthrough
    compareDags(args.displayMismatches, args.displayNumLargest,
                args.displayNumSmallest, stats, 'first')

    # Compare DAGs from second passthrough
    compareDags(args.displayMismatches, args.displayNumLargest,
                args.displayNumSmallest, stats, 'second')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Validation script for OptSched',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument('--verbose', '-v',
                        action='store_true', default=False,
                        dest='verbose',
                        help='Print the stats to terminal')

    # Run 1 directory for plaidbench
    parser.add_argument('directory1',
                        help='Directory containing a benchmark run')
    # Run 2 directory for plaidbench
    parser.add_argument('directory2',
                        help='Directory containing a benchmark run')

    # Option to display mismatches, defaults to off
    parser.add_argument('--mismatches', '-m', action='store_true',
                        default=False,
                        dest='displayMismatches',
                        help='Display mismatches')

    parser.add_argument('--largest', '-l', type=int,
                        default=10,
                        dest='displayNumLargest',
                        help='Print out x number of blocks with largest mismatches. Requires display mismatches.')

    parser.add_argument('--smallest', '-s', type=int,
                        default=50,
                        dest='displayNumSmallest',
                        help='Print out x number of mismatches with smallest number of instructions. Requires display mismatches.')

    parser.add_argument('--benchmark', '-b',
                        default='plaid',
                        choices=['plaid', 'shoc'],
                        dest='benchmark',
                        help='Select the benchmarking suite to parse for.')

    args = parser.parse_args()

    main(args)
